<!DOCTYPE html>
<html lang="en-us">
<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="theme" content="hugo-academic">
    <meta name="generator" content="Hugo 0.16" />
    <meta name="author" content="Taegyun Jeon">
    <meta name="description" content="Research Scientist in Machine Learning and Biomedical Engineering">

    <link rel="stylesheet" href="https://tgjeon.github.io/css/highlight.min.css">
    <link rel="stylesheet" href="https://tgjeon.github.io/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://tgjeon.github.io/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://tgjeon.github.io/css/academicons.min.css">
    <link rel="stylesheet" href="https://tgjeon.github.io/css/hugo-academic.css">
    


    <link rel="shortcut icon" href="https://tgjeon.github.io/img/favicon.ico" type="image/x-icon">
    <link rel="canonical" href="https://tgjeon.github.io/post/rnns-in-tensorflow/">

    <title>RNNs in Tensorflow, A Practical Guide and Undocumented Features | Taegyun Jeon</title>

</head>
<body id="top">


<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
    <div class="container">

        
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target=".navbar-collapse" aria-expanded="false">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://tgjeon.github.io/">Taegyun Jeon</a>
        </div>

        
        <div class="collapse navbar-collapse" id="#navbar-collapse-1">

            
            <ul class="nav navbar-nav navbar-right">
                
                <li class="nav-item"><a href="https://tgjeon.github.io/#top">Home</a></li>
                
                <li class="nav-item"><a href="https://tgjeon.github.io/#publications">Publications</a></li>
                
                <li class="nav-item"><a href="https://tgjeon.github.io/#posts">Posts</a></li>
                
                <li class="nav-item"><a href="https://tgjeon.github.io/#projects">Projects</a></li>
                
                <li class="nav-item"><a href="https://tgjeon.github.io/#contact">Contact</a></li>
                
            </ul>

        </div>
    </div>
</nav>

<div class="container">

    <article class="article" itemscope itemtype="http://schema.org/Article">
        <h1 itemprop="name">RNNs in Tensorflow, A Practical Guide and Undocumented Features</h1>

        

<div class="article-metadata">

    <span class="article-date">
        <time datetime="2016-08-23 12:54:32 &#43;0900 KST" itemprop="datePublished">Tue, Aug 23, 2016</time>
    </span>

    
    
    
    <span class="article-tags">
        <i class="fa fa-tags"></i>
        
        <a class="article-tag-link" href="https://tgjeon.github.io/tags/deep-learning">Deep Learning</a>, 
        
        <a class="article-tag-link" href="https://tgjeon.github.io/tags/translation">Translation</a>, 
        
        <a class="article-tag-link" href="https://tgjeon.github.io/tags/rnn">RNN</a>
        
    </span>
    
    

    
        
<ul class="share">
    <li>
        <a class="facebook" href="https://www.facebook.com/sharer.php?u=" target="_blank">
            <i class="fa fa-facebook-square"></i>
            <span class="hidden">Facebook</span>
        </a>
    </li>
    <li>
        <a class="twitter" href="https://twitter.com/intent/tweet?text=&amp;url=" target="_blank">
            <i class="fa fa-twitter-square"></i>
            <span class="hidden">Twitter</span>
        </a>
    </li>
    <li>
        <a class="linkedin" href="http://www.linkedin.com/shareArticle?mini=true&amp;url=&amp;title=" target="_blank">
            <i class="fa fa-linkedin-square"></i>
            <span class="hidden">LinkedIn</span>
        </a>
    </li>
    <li>
        <a class="email" href="mailto:?subject=&amp;body=">
            <i class="fa fa-envelope-square"></i>
            <span class="hidden">Email</span>
        </a>
    </li>
</ul>


    

</div>


        <div class="post" itemprop="articleBody">
            

<blockquote>
<p><a href="http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/">Original post</a> is written by Denny Britz (Google Brain team).</p>
</blockquote>

<p>이 포스트는 위 내용을 한글로 정리한 내용입니다.</p>

<hr />

<p>이전 <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/">튜토리얼</a>에서, Recurrent Neural Networks (RNNs)에 대한 이론들에 대해서 살펴보고, 간단한 RNN을 기초부터 만들어보는 과정을 진행했습니다. 이 과정들은 유용했지만, 실제로 RNNs에 대한 고수준의 기초요소들을 제공하는 Tensorflow와 같이 라이브러리를 사용해봅시다.</p>

<p>RNNs을 함수 호출해서 사용하는 것처럼 쉽게 쓰면 좋겠지만, 현실은 만만치 않습니다. 이번 포스트에서는 Tensorflow를 이용하여 RNNs을 실질적으로 동작시켜보고, 특히 공식 사이트에서 문서화 되지 않은 기능들에 대해 다뤄보도록 하겠습니다.</p>

<p>본 포스트는 Jupyer 노트북이 포함된 Github 저장소에 아래와 같은 예제들을 다루고 있습니다.</p>

<ul>
<li><a href="https://github.com/dennybritz/tf-rnn/blob/master/sequence_example.ipynb">tf.SequenceExample 사용하기</a></li>
<li><a href="https://github.com/dennybritz/tf-rnn/blob/master/batching_padding.ipynb">Batching과 Padding</a></li>
<li><a href="https://github.com/dennybritz/tf-rnn/blob/master/dynamic_rnn.ipynb">Dynamic RNN</a></li>
<li><a href="https://github.com/dennybritz/tf-rnn/blob/master/bidirectional_rnn.ipynb">Bidirectional Dynamic RNN</a></li>
<li><a href="https://github.com/dennybritz/tf-rnn/blob/master/rnn_cell_wrappers.py.ipynb">RNN Cells과 Cell Wrappers</a></li>
<li><a href="https://github.com/dennybritz/tf-rnn/blob/master/loss_masking.py.ipynb">Masking the Loss</a></li>
</ul>

<h3 id="데이터-전처리-tf-sequenceexample-사용하기">데이터 전처리: tf.SequenceExample 사용하기</h3>

<p><strong><a href="https://github.com/dennybritz/tf-rnn/blob/master/sequence_example.ipynb">tf.SequenceExample 주피터 노트북을 확인하세요!</a></strong></p>

<p>RNNs은 순차적인 데이터를 사용하며, 이 순차적인 데이터는 입력과 출력을 여러 시간 스텝으로 구성되어 있습니다. Tensorflow는 순차적 데이터: <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto">tf.SequenceExample</a> 를 다루기 위해 <a href="https://developers.google.com/protocol-buffers/">프로토콜 버퍼</a>가 정의되어 있습니다.</p>

<p>사용자는 Python/Numpy 배열로부터 직접 데이터를 불러올 수 있습니다. 하지만, <code>tf.SequenceExample</code>가 더 관심 대상이므로 이것을 사용해보도록 합시다. 이 자료 구조는 비순차적 특징 (non-sequential features)은 <code>&quot;context&quot;</code>로, 순차적 특징 (sequential features)은 <code>&quot;feature_lists&quot;</code>로 구성됩니다. 장황하긴 하지만, 이것이 주는 장점은 아래와 같습니다.</p>

<ul>
<li><strong>쉬운 <a href="https://www.tensorflow.org/versions/r0.10/how_tos/distributed/index.html#distributed-tensorflow">분산 학습</a></strong>: 데이터를 여러개의 SequenceExamples을 포함하는 <code>TFRecord</code> 파일들로 나누고, Tensorflow에 탑재된 분산 학습 기능을 이용합니다.</li>
<li><strong>재사용성 (Reusability)</strong>: 다른 사용자들도 본인이 만든 모델을 가져다가 자신들의 데이터를 <code>tf.SequenceExample</code> 형태로 바꿔서 사용할 수 있습니다. 모델 코드 부분을 수정할 필요가 없습니다.</li>
<li><strong>Tensorflow의 데이터 불러오기 파이프라인 함수 사용</strong>: <code>tf.parse_single_sequence_example</code>과 같이 사용 가능합니다. <code>tf.learn</code>과 같은 라이브러리 역시 데이터 입력을 프로토콜 버퍼 포맷으로 입력될 것을 예상하여, 간편한 함수를 지원합니다.</li>
<li><strong>데이터 전처리와 모델 코드의 분리</strong>: <code>tf.SequenceExample</code>을 이용하게 되면, 사용자로 하여금 데이터 전처리와 Tensorflow 모델 코드 부분을 분리하도록 합니다. 이것은 소스코드 작성에 매우 유익한 부분이며, 입력 데이터가 어떤 형태로 들어올지 가정하지 않아도 됩니다.</li>
</ul>

<p>실제로, 사용자가 데이터를 <code>tf.SequenceExample</code> 포맷으로 변환하는 작은 스크립트를 작성하고, 그다음 하나 혹은 여러개의 <code>TFRecord</code> 파일을 작성합니다. 이 <code>TFRecord</code>파일은 Tensorflow가 파싱 (parsing)하게 되고, 사용자 모델의 입력이 됩니다:</p>

<ol>
<li>사용자의 데이터를 <code>tf.SequenceExample</code> 포맷으로 변환합니다.</li>
<li>serialized 데이터를 <code>TFRecord</code>로 하나 혹은 여러개 파일로 작성합니다.</li>
<li><a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#TFRecordReader"><code>tf.TFRecordReader</code></a>를 이용하여 examples을 파일로부터 읽어옵니다.</li>
<li>각 example을 <a href="https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/python/ops/parsing_ops.py"><code>tf.parse_single_sequence_example</code></a>를 이용해 파싱합니다. (아직 공식 문서화되지 않은 기능입니다.)</li>
</ol>

<pre><code class="language-python">sequences = [[1, 2, 3], [4, 5, 1], [1, 2]]
label_sequences = [[0, 1, 0], [1, 0, 0], [1, 1]]
 
def make_example(sequence, labels):
    # 결과값은 example입니다.
    ex = tf.train.SequenceExample()
    # non-sequential feature 부분입니다. (데이터를 설명하는 정보)
    sequence_length = len(sequence)
    ex.context.feature[&quot;length&quot;].int64_list.value.append(sequence_length)
    # sequential features 부분인 feature_lists 부분입니다. (실제 데이터)
    fl_tokens = ex.feature_lists.feature_list[&quot;tokens&quot;]
    fl_labels = ex.feature_lists.feature_list[&quot;labels&quot;]
    for token, label in zip(sequence, labels):
        fl_tokens.feature.add().int64_list.value.append(token)
        fl_labels.feature.add().int64_list.value.append(label)
    return ex
 
# 모든 examples을 하나의 TFRecord파일로 작성합니다.
with tempfile.NamedTemporaryFile() as fp:
    writer = tf.python_io.TFRecordWriter(fp.name)
    for sequence, label_sequence in zip(sequences, label_sequences):
        ex = make_example(sequence, label_sequence)
        writer.write(ex.SerializeToString())
    writer.close()
</code></pre>

<p>그리고, example을 파싱하여 사용합니다:</p>

<pre><code class="language-python"># 하나의 serialized example입니다.
# (TFRecordReader를 이용하여 하나의 example을 읽어올 수 있습니다.)
ex = make_example([1, 2, 3], [0, 1, 0]).SerializeToString()
 
# example을 파싱하는 방식을 정의합니다. (이름과 데이터 타입)
context_features = {
    &quot;length&quot;: tf.FixedLenFeature([], dtype=tf.int64)
}
sequence_features = {
    &quot;tokens&quot;: tf.FixedLenSequenceFeature([], dtype=tf.int64),
    &quot;labels&quot;: tf.FixedLenSequenceFeature([], dtype=tf.int64)
}
 
# example로 부터 파싱합니다.
context_parsed, sequence_parsed = tf.parse_single_sequence_example(
    serialized=ex,
    context_features=context_features,
    sequence_features=sequence_features
)
</code></pre>

<h3 id="batching과-데이터-padding">Batching과 데이터 Padding</h3>

<p><strong><a href="https://github.com/dennybritz/tf-rnn/blob/master/batching_padding.ipynb">Batching and Padding 주피터 노트북을 확인하세요!</a></strong></p>

<p>Tensorflow의 RNN 함수는 $[B, T, &hellip;]$ 형태의 tensor를 입력으로 기대합니다. 여기서 $B$는 배치 사이즈, $T$는 각 입력의 시간 길이를 의미합니다. (예. 한 문장의 단어 수). 그리고 마지막 dimension은 사용자의 데이터와 관련있습니다. 문제를 눈치채셨나요? 보편적으로, 하나의 배치에 포함된 모드 sequence들이 길이 $T$와 같지 않다는 것입니다. 하지만 RNN 모델에 입력으로 집어넣어주기 위해서는 길이를 맞춰줘야 합니다. 보통은 padding을 통해 해결합니다: 각 example에 0을 채워 넣어서 sequence의 길이를 같게 맞춰줍니다.</p>

<p>만약, 사용자의 sequence중 하나의 길이가 1000이라고 합시다. 하지만 sequences의 평균 길이가 20이라고 한다면 어떨까요? 만약 모든 example에 대해 길이 1000개에 맞춰서 padding을 하게 되면, 엄청난 공간 (뿐만 아니라 계산 시간)의 낭비가 발생합니다. 바로 여기서 batch padding이 필요합니다. 만약 사용자가 32개의 배치들을 만들었다면, 단지 각 배치에 있는 examples을 같은 크기 (해당 배치의 최대 길이의 example 크기에 맞춰서)로만 padding 시키면 되는 것입니다. 이 방식대로라면, 정말 긴 example은 오직 하나의 배치에만 영향을 미칩니다. 다른 모든 배치의 example 데이터들은 영향에서 벗어날 수 있습니다.</p>

<p>실제로 다루기엔 지저분해보이지만, 다행스럽게도 Tensorflow에는 batch padding 기능을 포함하고 있습니다. 만약 사용자가 <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#batch"><code>tf.train.batch</code></a>를 호출할 때 <code>dynamic_pad=True</code>로 설정한다면, 반환되는 batch 결과값들은 자동적으로 0이 padding됩니다. 편리하네요! lower-level 옵션은 <a href="https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#PaddingFIFOQueue"><code>tf.PaddingFIFOQueue</code></a>를 사용하는 것입니다.</p>

<h4 id="side-note-단어-vocabulary-종류-class-를-사용할때는-0으로-padding-하는-것을-주의해서-쓰세요">Side note: 단어(Vocabulary)/종류(class)를 사용할때는 0으로 padding 하는 것을 주의해서 쓰세요!</h4>

<p>만약 사용자가 분류 (classification) 문제를 다루고 있고, 입력 tensor가 class IDs (0, 1, 2, &hellip;)를 포함한다면, padding을 사용할때 주의해야 합니다. tensor를 0으로 padding 한다면, 0-padding과 &ldquo;class 0&rdquo;과의 차이점을 구분할 수 없는 경우가 생깁니다. 사용자 모델이 다루고자 하는 문제에 따라 달라지겠지만, 이 이슈에 대해서 안전하고 싶다면 &ldquo;class 0&rdquo; 대신 &ldquo;class 1&rdquo;로 부터 시작하도록 사용해보세요. (이 예제가 masking the loss function 부분에서 문제를 발생시킵니다. 자세한 내용은 아래에서 설명하도록 하겠습니다.)</p>

<pre><code class="language-python"># [0, 1, 2, 3, 4 ,...]
x = tf.range(1, 10, name=&quot;x&quot;)
 
# A queue that outputs 0,1,2,3,...
range_q = tf.train.range_input_producer(limit=5, shuffle=False)
slice_end = range_q.dequeue()
 
# Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....
y = tf.slice(x, [0], [slice_end], name=&quot;y&quot;)
 
# Batch the variable length tensor with dynamic padding
batched_data = tf.train.batch(
    tensors=[y],
    batch_size=5,
    dynamic_pad=True,
    name=&quot;y_batch&quot;
)
 
# Run the graph
# tf.contrib.learn takes care of starting the queues for us
res = tf.contrib.learn.run_n({&quot;y&quot;: batched_data}, n=1, feed_dict=None)
 
# Print the result
print(&quot;Batch shape: {}&quot;.format(res[0][&quot;y&quot;].shape))
print(res[0][&quot;y&quot;])
</code></pre>

<p>다음과 같은 결과를 나타냅니다:</p>

<pre><code class="language-python">Batch shape: (5, 4)
[[0 0 0 0]
 [1 0 0 0]
 [1 2 0 0]
 [1 2 3 0]
 [1 2 3 4]]
</code></pre>

<p>그리고 <code>PaddingFIFOQueue</code>를 사용한 결과도 동일합니다.</p>

<pre><code class="language-python"># ... Same as above
 
# Creating a new queue
padding_q = tf.PaddingFIFOQueue(
    capacity=10,
    dtypes=tf.int32,
    shapes=[[None]])
 
# Enqueue the examples
enqueue_op = padding_q.enqueue([y])
 
# Add the queue runner to the graph
qr = tf.train.QueueRunner(padding_q, [enqueue_op])
tf.train.add_queue_runner(qr)
 
# Dequeue padded data
batched_data = padding_q.dequeue_many(5)
 
# ... Same as above
</code></pre>

<h3 id="rnn과-dynamic-rnn">RNN과 Dynamic_RNN</h3>

<p><strong><a href="https://github.com/dennybritz/tf-rnn/blob/master/dynamic_rnn.ipynb">DYNAMIC_RNN 주피터 노트북을 확인하세요!</a></strong></p>

<p>Tensorflow는 두가지 종류의 RNNs을 제공하고 있습니다: <code>tf.nn.rnn</code>과 <code>tf.nn.dynamic_rnn</code>. 무엇을 사용할까요?</p>

<p>내부적으로, <code>tf.nn.rnn</code>은 고정된 길이를 가지는 RNN을 펼친 그래프를 생성합니다. 만약 200개의 시간 스텝을 입력으로 하여 <code>tf.nn.rnn</code>을 사용한다면, 200개의 RNN 스텝을 가지는 고정된 (static) 그래프가 만들어 집니다. 첫째, 그래프의 생성은 느립니다. 둘째, 원래 정의했던 길이보다 더 긴 길이 (200개 이상)의 sequence를 처리하지 못합니다.</p>

<p><code>tf.nn.dynamic_rnn</code>은 이 문제들을 해결할 수 있습니다. <code>tf.while</code> loop를 이용하여, 실행될때 동적으로 그래프를 생성합니다. 이를 통해 빠르게 그래프를 생성하고, 다양한 길이의 배치들을 입력으로 처리 할 수 있습니다. 성능은 어떨까요? 일반적으로 생각하기에 static RNN이 그래프를 미리 생성해두기 때문에, dynamic RNN보다 빠를것이라고 생각합니다. 하지만, 제 경험상 그렇지 않았습니다.</p>

<p>간단히 얘기하면, 그냥 <strong><code>tf.nn.dynamic_rnn</code></strong>을 쓰세요. <code>tf.nn.rnn</code>을 사용해서 얻을수 있는 장점이 없습니다. <code>tf.nn.rnn</code>이 앞으로 사라져도 놀랍지 않습니다.</p>

<h3 id="sequence-길이를-rnn에-전달하기">Sequence 길이를 RNN에 전달하기</h3>

<p><strong><a href="https://github.com/dennybritz/tf-rnn/blob/master/bidirectional_rnn.ipynb">Bidirectional RNN 주피터 노트북을 확인하세요!</a></strong></p>

<p>Padding 된 입력을 Tensorflow의 RNN 함수들을 이용할 때, <code>sequence_length</code> 파라미터를 전달해 주는 것이 중요합니다. 개인적인 생각으로 이 파라미터는 선택적으로 쓰기보단, 필수적으로 꼭 사용하는게 좋습니다. <code>sequence_length</code>는 두가지 효과가 있습니다. 1. 계산시간을 절약해줍니다. 2. 정확함을 보장해줍니다.</p>

<p>2개의 example을 가진 배치가 있다고 합시다. example 의 길이는 13이고, 다른 example의 길이는 20입니다. 길이가 13인 example은 0으로 채워져서 (0-padded) 길이가 20이 됩니다. 각각은 128개의 숫자로 된 벡터입니다. 그렇다면 RNN tensor의 모양은 <strong>[2, 20, 128]</strong>입니다. dynamic_rnn 함수는 (outputs, state) 형태의 튜플을 반환합니다. 여기서 <strong>output</strong>은 <strong>[2, 20, &hellip;]</strong> 형태의 tensor 이며, 마지막 차원은 각 시간 스텝의 RNN 결과입니다. <strong>state</strong>는 각 example에 대한 최종 상태이며, tensor 크기는 <strong>[2, &hellip;]</strong>입니다. 마지막 차원의 값은 사용하는 RNN cell에 따라 달라집니다.</p>

<p>여기서 문제가 발생합니다. 시간 스텝 13에 도달 했을 때, 배치의 첫번째 example (총 길이 (20) = 본래 길이 (13) + 0채우기 (7))은 이미 &ldquo;완료&rdquo;된 상태고, 사용자는 더 이상 추가적인 계산이 이루어지길 원하지 않습니다. 두번째 example은 상황이 다르므로 (총 길이 (20) = 본래 길이 (20)), 시간 스텝 20까지 RNN이 계산해야합니다. <strong>sequence_length=[13, 20]</strong>을 전달한다면, 사용자는 Tensorflow에게 첫번째 example은 시간 스텝 13까지만 계산하고,</p>

        </div>
    </article>

    <nav>
    <ul class="pager">
        
        <li class="previous"><a href="https://tgjeon.github.io/post/rnn_rnn/"><span aria-hidden="true">&larr;</span> Recurrent Neural Networks</a></li>
        

        
    </ul>
</nav>

    
<section id="comments">
    <div id="disqus_thread">
        <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'tgjeon';
    var disqus_identifier = 'https:\/\/tgjeon.github.io\/post\/rnns-in-tensorflow\/';
    var disqus_title = 'RNNs in Tensorflow, A Practical Guide and Undocumented Features';
    var disqus_url = 'https:\/\/tgjeon.github.io\/post\/rnns-in-tensorflow\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
</section>



</div>
<footer class="site-footer">
    <div class="container">
        <p class="powered-by">

            &copy; 2016 Taegyun Jeon &middot; 

            Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

            <span class="pull-right"><a href="#" id="back_to_top"><span class="button_icon"><i class="fa fa-chevron-up fa-2x" aria-hidden="true"></i></span></a></span>

        </p>
    </div>
</footer>

        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
        <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
        <script src="https://tgjeon.github.io/js/jquery-1.12.3.min.js"></script>
        <script src="https://tgjeon.github.io/js/bootstrap.min.js"></script>
        <script src="https://tgjeon.github.io/js/hugo-academic.js"></script>
        

        
        <script>
            (function(i,s,o,g,r,a,m){
                i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},
                    i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;
                m.parentNode.insertBefore(a,m)
            })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');
            ga('create', 'UA-77919576-1', 'auto');
            ga('set', 'anonymizeIp', true);
            ga('send', 'pageview');

             
            var links = document.querySelectorAll('a');
            Array.prototype.map.call(links, function(item) {
                if (item.host != document.location.host) {
                    item.addEventListener('click', function() {
                        var action = item.getAttribute('data-action') || 'follow';
                        ga('send', 'event', 'outbound', action, item.href);
                    });
                }
            });
        </script>
        

        
        <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.3.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script>

        
        
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
        </script>
        <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
        

    </body>
</html>

