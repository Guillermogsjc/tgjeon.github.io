<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Taegyun Jeon</title>
    <link>https://tgjeon.github.io/</link>
    <description>Recent content on Taegyun Jeon</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Taegyun Jeon</copyright>
    <lastBuildDate>Wed, 29 Jun 2016 10:49:16 +0900</lastBuildDate>
    <atom:link href="https://tgjeon.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Stochastic Gradient Methods</title>
      <link>https://tgjeon.github.io/post/stochastic_gradient_methods/</link>
      <pubDate>Wed, 29 Jun 2016 10:49:16 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/post/stochastic_gradient_methods/</guid>
      <description>

&lt;h1 id=&#34;stochastic-gradient-methods-for-large-scale-machine-learning-part-1:d651e37548ed21cc65094b1e7112d7f5&#34;&gt;Stochastic Gradient Methods for Large-Scale Machine Learning (Part 1)&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Original presentation materials: &lt;a href=&#34;http://www.icml.cc/2016/tutorials/part-1.pdf&#34;&gt;Part1&lt;/a&gt;, &lt;a href=&#34;http://www.icml.cc/2016/tutorials/part-2.pdf&#34;&gt;Part2&lt;/a&gt;, &lt;a href=&#34;http://www.icml.cc/2016/tutorials/part-3.pdf&#34;&gt;Part3&lt;/a&gt; is written by Leon Bottou (Facebook AI Research), Frank E. Curtis (Lehigh University), and Jorge Nocedal (Northwestern University).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 포스트는 ICML 2016 Tutorial 중 &amp;ldquo;Stochastic Gradient Methods for Large-Scale Machine Learning&amp;rdquo; 의 내용을 한글로 정리한 포스트입니다. 해당 발표자료는 위 링크를 참고하여 확인하시기 바랍니다. 원 튜토리얼 자료와 같이 보시는 것을 권합니다.&lt;/p&gt;

&lt;p&gt;해당 튜토리얼은 &lt;a href=&#34;http://arxiv.org/abs/1606.04838&#34;&gt;&amp;ldquo;Optimization Methods for Large-Scale Machine Learning&amp;rdquo;&lt;/a&gt;, L. Bottou, F.E. Curtis, J. Nocedal, Prepared for SIAM Review 논문을 요약한 내용이라고 합니다. 다 자세한 내용을 보고싶으신 분은 위 논문을 참고하시기 바랍니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;튜토리얼의-목표:d651e37548ed21cc65094b1e7112d7f5&#34;&gt;튜토리얼의 목표&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Stochastic gradient (SG) 방법에 대해 알아본다.&lt;/li&gt;
&lt;li&gt;SG는 왜 중요해질까?&lt;/li&gt;
&lt;li&gt;핵심 매커니즘은 무엇인가?&lt;/li&gt;
&lt;li&gt;convex와 non-convex의 경우에 어떻게 행동한다고 할수 있는가?&lt;/li&gt;
&lt;li&gt;SG를 향상 시키기위해서 어떤 노력들이 있었는가?&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;튜토리얼의-구성:d651e37548ed21cc65094b1e7112d7f5&#34;&gt;튜토리얼의 구성&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;Motivation for the stochastic gradient (SG) method&lt;/li&gt;
&lt;li&gt;Analysis of SG&lt;/li&gt;
&lt;li&gt;Beyond SG&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;문제-정의-problem-statement:d651e37548ed21cc65094b1e7112d7f5&#34;&gt;문제 정의 (Problem statement)&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;학습 데이터가 주어집니다. training set: $ {(x_1, y_1), &amp;hellip;, (x_n, y_n)} $&lt;/li&gt;
&lt;li&gt;손실 함수가 주어집니다. loss function: $ \ell(h,y) $

&lt;ul&gt;
&lt;li&gt;여기서 우리의 예측이 실제 학습 데이터의 label인 $y$와 얼마나 다른지 측정합니다.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;예측 함수는 $h(x;w)$ 입니다.

&lt;ul&gt;
&lt;li&gt;$x$는 입력 데이터 (input data), $w$는 학습 모델의 가중치 (weights of model) 입니다. 모델 $w$가 입력 $x$을 받아 예측 결과 $\hat {y}$를 출력으로 나타냅니다.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기서 우리의 목표는 가장 최선의 예측을 하는 것입니다. 주어진 학습 데이터에 대해 예측 손실을 최소화하는 모델을 만들어서 이 목표를 달성하고자 합니다. 하나의 수식으로 표현하면 아래와 같습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$ \min _{w}{\frac {1}{n} \sum _{i=1}^{n}{\ell(h(x_i; w), y_i)} } $&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;모든 학습 데이터에 대해 예측을 하고, 각 학습 데이터에 대한 예측이 학습 데이터의 label인 $y$와 차이가 적게 나타도록 하는 $w$가 우리의 학습 모델이 됩니다. 충분한 학습 데이터는 경험이라고 표현 할 수 있습니다. 우리의 학습 모델이 얼마나 큰 위험 요소 (Risk)를 가지고 있는지 표현해봅시다. 여기서 $f$는 손실 함수 (loss function) 입니다. 경험적 위험요소 (empirical risk)는 아래와 같이 표현됩니다. 각 학습 데이터 마다 loss를 계산해서 평균내어 risk로 나타냅니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;empirical risk: $R_n(w) = \frac{1}{n} \sum _{i=1}^{n}{f_i(w)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;데이터가 충분히 모였다면 데이터에 대한 확률 분포가 존재합니다. 이 때, 우리의 학습 데이터를 랜덤 변수 $\xi=(x_i, y_i)$ 로 표현합니다. 그렇다면 위험요소 (risk)는 기대값으로 나타낼 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;expected risk: $R(w) = E[f(w;x_i)] $&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;stochastic-gradient-method-vs-batch-gradient-method:d651e37548ed21cc65094b1e7112d7f5&#34;&gt;Stochastic Gradient Method vs Batch Gradient Method&lt;/h3&gt;

&lt;p&gt;이전에 표현했던 empirical risk minimization을 살펴봅시다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;empirical risk: $R_n(w) = \frac{1}{n} \sum _{i=1}^{n}{f_i(w)}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;데이터를 하나씩 관찰하면서, 모델의 가중치는 아래와 같은 수식을 통해 변화됩니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;weight update: $w_{k+1} = w_k - \alpha_k \nabla f_i(w_k),$ where $i \in$ {$1,&amp;hellip;,n$} choose at random

&lt;ul&gt;
&lt;li&gt;단순 반복을 통해 가능합니다. 데이터 1개씩만 보고 변화율 (gradient)을 변경합니다.&lt;/li&gt;
&lt;li&gt;i번째 데이터를 고르는 순서에 따라 확률 과정 (stochastic process)이 결정됩니다.&lt;/li&gt;
&lt;li&gt;gradient descent method가 아닙니다.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://projecteuclid.org/euclid.aoms/1177729586&#34;&gt;[Robibins-Monro&amp;rsquo;51]&lt;/a&gt;: H. Robbins and S. Monro. A Stochastic Approximation Method. Ann. Math. Statist. 22 (1951), no. 3, 400&amp;ndash;407.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그렇다면, batch gradient method로 넘어가봅시다. 일정 묶음의 수만큼 데이터를 확인하고, loss를 계산하여 risk를 모델 가중치 변화에 적용 시킵니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;batch gradient method: $w_{k+1} = w_k - \alpha_k \nabla R_n(w_k)$&lt;/li&gt;
&lt;li&gt;$w_{k+1} = w_k - \frac {\alpha_k}{n} \sum _{i=1}^{n}{\nabla f_i(w_k)}$

&lt;ul&gt;
&lt;li&gt;계산량이 늘어나게 됩니다.&lt;/li&gt;
&lt;li&gt;다양한 최적화 알고리즘을 선택하여 사용할 수 있습니다.&lt;/li&gt;
&lt;li&gt;각 배치 별로 병렬처리가 가능해집니다.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그럼 왜 stochastic gradient (SG) 기법이 더 탁월하다고 볼 수 있을까요? &lt;strong&gt;여기서 Risk ($R$)를 최적하 하기 위한 stochastic 기법과 batch 기법의 계산량의 상호보완적인 내용을 짚고 넘어가야 합니다.&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&#34;sg-bg:d651e37548ed21cc65094b1e7112d7f5&#34;&gt;SG &amp;gt; BG&lt;/h3&gt;

&lt;p&gt;stochastic gradient (SG) 기법은 batch 기법보다 더 효율적입니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;생각해 볼 문제 #1

&lt;ul&gt;
&lt;li&gt;만약 집합 S의 10개의 복사본으로 데이터가 구성된다면,&lt;/li&gt;
&lt;li&gt;batch 기법으로 수행한다면 SG 기법의 경우보다 10배의 계산량이 요구됩니다.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;생각해 볼 문제 #2

&lt;ul&gt;
&lt;li&gt;데이터를 구성할 때, training set (40%), test set (30%), validation set (30%) 식으로 보통 구성합니다.&lt;/li&gt;
&lt;li&gt;20%, 10%, 혹은 1% 이런식의 구성은 의미가 없을까요?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://www.icml.cc/2016/tutorials/part-1.pdf&#34;&gt;Part1&lt;/a&gt;, slide#9를 보면 LBFGS와 SGD의 실질적인 비교 결과를 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) algorithm:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;quasi-newton method와 유사한 최적화 기법입니다.&lt;/li&gt;
&lt;li&gt;제한된 메모리를 가진 컴퓨터에서 Broyden-Fletcher-Goldfarb-Shanno (BFGS) 알고리즘을 근사화 (approximation)시킨 기법입니다.&lt;/li&gt;
&lt;li&gt;기계 학습에서 파라미터 추정 (parameter estimation)에서 많이 쓰입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Slide#9에서, 초기 과정에서 SGD가 LBFGS보다 월등히 빠르게 수렴하는 것을 확인 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Slide#10에서는 Quadratic One-Dimensional example을 소개하고 있습니다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$ \min _{x \in \Re}\sum _{i=1}^{m} {(a_i x - b_i)}^{2} $&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위 예제에서 처럼 혼동되는 구역 (region of confusion)에서 2차식을 임의로 찾아서 목표로 하는 리스크 감소시키는 것은 어려운 문제입니다.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust Physiological Signal Analysis: Design and Applications with Machine Learning</title>
      <link>https://tgjeon.github.io/publication/robust-physiological-signal-analysis/</link>
      <pubDate>Wed, 22 Jun 2016 10:44:05 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/robust-physiological-signal-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unfolding Computational Graphs</title>
      <link>https://tgjeon.github.io/post/rnn_unfolding_computational_graph/</link>
      <pubDate>Thu, 02 Jun 2016 00:59:05 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/post/rnn_unfolding_computational_graph/</guid>
      <description>

&lt;h3 id=&#34;chapter-10-sequence-modeling-recurrent-and-recursive-nets:af5079f711a612e7f7c8a351ea7e20f6&#34;&gt;Chapter 10. Sequence Modeling: Recurrent and Recursive Nets&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://www.deeplearningbook.org&#34;&gt;Original book chapter&lt;/a&gt; is written by Ian Goodfellow, Yoshua Bengio, and Aaron Courville.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 포스트는 &amp;ldquo;Deep Learning published by Mit Press (2016)&amp;rdquo; 의 Recurrent Neural Networks에 해당되는 내용을 한글로 정리한 내용입니다. 해당 그림은 저작권 문제로 위 링크를 참고하여 확인하시기 바랍니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;unfolding-computational-graphs:af5079f711a612e7f7c8a351ea7e20f6&#34;&gt;Unfolding Computational Graphs&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Classical Dynamic System&lt;/strong&gt;:
$ S^{ (t) }=f(s^{ (t-1) };\theta ) $, 여기서 $s^{(t)}$ 는 시스템의 상태를 나타낸다.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Dynamical system driven by an external signal&lt;/strong&gt; $ x^{(t)} $:
$ S^{ (t) }=f(s^{ (t-1) }, x^{(t)} ;\theta ) $&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recurrent Neural Networks (RNN) 은 다양한 방법으로 만들수 있다. 대부분은 feedforward neural network로 간주되며, recurrence (재귀표현)를 포함하고 있다면 Recurrent Neural Networks (RNN) 이라고 할 수 있다.&lt;/p&gt;

&lt;p&gt;앞서 나온 &lt;strong&gt;Dynamical System&lt;/strong&gt; 식을 이용해, 다음과 같이 유사한 형태의 식으로 RNN의 hidden unit을 정의한다.
$$ h^{(t)} = f(h^{ (t-1) }, x^{(t)} ;\theta ) $$&lt;/p&gt;

&lt;p&gt;예를 들어, RNN이 과거 데이터를 학습하고, 미래를 예측하는 일을 수행한다고 하자.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;입력: $ x^{(t)}$ (시점 $t$ 까지의 입력)&lt;/li&gt;
&lt;li&gt;종합: $ h^{(t)}$ (정보 손실이 있는 종합된 내용)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 과정을 통해 과거 정보에서 선택적으로 필요한 정보만 유지하여 종합한다. 예를 들어, RNN이 통계적 언어 모델링을 통해, 과거의 단어들을 바탕으로 다음에 나올 단어를 예측한다. 이를 위해, 시점 (time step) $t$ 까지 모든 입력을 다 기억할 필요는 없다. 일정한 정보만으로도 문장의 나머지 단어들을 예측하기에 충분하다.&lt;/p&gt;

&lt;p&gt;RNN을 그림으로 묘사하는 두가지 방법이 있다.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Folded graph&lt;/code&gt;: 모든 기능을 하나의 노드로만 표현.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Unfolded graph&lt;/code&gt;: 각 기능들이 각각의 노드로 표현. Unfolded 표현은 sequence 길이와 연관된 크기를 가진다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Unfolding 과정은 다음과 같은 장점을 지닌다.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sequence 길이와 상관없이, 학습된 모델은 항상 같은 입력 크기를 가진다.&lt;/li&gt;
&lt;li&gt;같은 파라미터를 사용하는 전이 함수 (transition function) $f$ 를 사용할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 두가지 요인 때문에, 각 시점마다 $g^{(t)}$ 를 개별적으로 학습시키지 않아도 된다. 같은 파리미터가 공유되기 때문에, 단일 모델 $f$ 가 모든 sequence 길이와 모든 시점에서 동작한다.&lt;/p&gt;

&lt;p&gt;단일화, 공유되는 모델 학습은 일반화를 가능하게 한다.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;학습 데이터상의 sequence 길이에 무관하다.&lt;/li&gt;
&lt;li&gt;적은 학습 데이터로도 예측이 가능하다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;두가지 RNN 표현법은 다음과 같이 각자의 용도가 있다.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Folded graph (recurrent graph)&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;간단명료하다.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Unfolded graph&lt;/code&gt;

&lt;ul&gt;
&lt;li&gt;각 단계에서 계산 수행을 분명하게 기술한다.&lt;/li&gt;
&lt;li&gt;진행 방향에 따라 아이디어를 묘사하기 쉽다.&lt;/li&gt;
&lt;li&gt;정보의 흐름 (Forward): output과 loss의 계산&lt;/li&gt;
&lt;li&gt;정보의 흐름 (Backward): gradient 계산&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Living Energy: distributed-intelligence management and operation for smart energy</title>
      <link>https://tgjeon.github.io/project/living_energy/</link>
      <pubDate>Thu, 19 May 2016 10:54:56 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/project/living_energy/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Sponsor&lt;/strong&gt;: Gwangu Institute of Science and Technology (GIST)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Period&lt;/strong&gt;: Jan 2016 - Dec 2017&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Development Environment&lt;/strong&gt;: Linux, Python&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tasks&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Develop perversive demand response (DR) based on Big Data analytics.&lt;/li&gt;
&lt;li&gt;Optimize electricity load forecasting with energy patterns and factor diversification.&lt;/li&gt;
&lt;li&gt;Drive a long-term analysis and reporting based on load forecasting and electricity consumptions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Knowledge and Skills&lt;/strong&gt;
&lt;code&gt;Linux&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;TensorFlow&lt;/code&gt;, &lt;code&gt;Recurrent Neural Networks&lt;/code&gt;, &lt;code&gt;Load Forecasting&lt;/code&gt;, &lt;code&gt;Energy Price Forecasting&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Related Works&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[kaggle] Digit Recognizer using Convolutional Neural Networks for MNIST dataset</title>
      <link>https://tgjeon.github.io/project/cnn_for_mnist/</link>
      <pubDate>Wed, 04 May 2016 10:29:55 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/project/cnn_for_mnist/</guid>
      <description>

&lt;h3 id=&#34;project-information:4e0834f1f8e7b3edf13b26b2607f7c40&#34;&gt;Project Information&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Sponsor&lt;/strong&gt;: Individual project&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Period&lt;/strong&gt;: May, 2016&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Task&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Classification of images of handwritten single digit.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Related Knowledge&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Digit Recognition&lt;/code&gt;, &lt;code&gt;Computer Vision&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Skills&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;TensorFlow&lt;/code&gt;, &lt;code&gt;Convolutional Neural Networks&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Report&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Detail explanations are on the &lt;a href=&#34;https://tgjeon.github.io/kaggle_MNIST/&#34;&gt;project page&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;descriptions:4e0834f1f8e7b3edf13b26b2607f7c40&#34;&gt;Descriptions&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Description and contributions&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In this individual project, I show how TensorFlow can be used to implement the Convolutional Neural Networks. I introduce a short descriptions of the model, which serves both as a refresher but also as to anchor the notation and show how mathematical expressions are mapped onto TensorFlow graph.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tgjeon.github.io/img/mnist.png&#34; alt=&#34;MNIST&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning in a Nutshell: Core Concepts</title>
      <link>https://tgjeon.github.io/post/deep_learning_in_a_nut_shell_core_concept/</link>
      <pubDate>Sat, 23 Apr 2016 16:58:51 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/post/deep_learning_in_a_nut_shell_core_concept/</guid>
      <description>

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts&#34;&gt;Original post&lt;/a&gt; is written by Tim Dettmers&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 포스트는 &lt;a href=&#34;https://devblogs.nvidia.com/parallelforall&#34;&gt;Parallel ForAll&lt;/a&gt; 에 작성하는 시리즈 중 첫 글이며, &lt;a href=&#34;https://developer.nvidia.com/deep-learning&#34;&gt;딥 러닝&lt;/a&gt;에 대해 직관적이고 가볍게 소개하고자 한다. 본 포스트는 딥 러닝의 가장 중요한 개념을 다루고 있으며, 수학적 이론 지식보다 기본적인 개념의 전달을 목적으로 한다. 수식과 함께라면 더 깊은 이해가 가능하겠지만, 이 포스트는 비유와 그림을 통해 더욱 이해하기 쉬운 직관적인 개요를 전달하고자 한다.
이 글들은 단어사전식으로 작성되어 딥러닝 개념을 위한 참고자료로 사용 될 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts&#34;&gt;Part 1&lt;/a&gt;에서는 딥 러닝의 중요한 개념에 대해서 소개한다. &lt;a href=&#34;https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training&#34;&gt;Part 2&lt;/a&gt;에서는 딥 러닝의 역사적 배경과 학습 과정, 알고리즘, 실용적인 기법 등을 살펴본다. &lt;a href=&#34;https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-sequence-learning&#34;&gt;Part 3&lt;/a&gt;에서는 자연어 번역을 위한 sequence learning 에 대해 알아본다. recurrent neural networks, LSTMs, encoder-decoder system을 포함한다.&lt;/p&gt;

&lt;h2 id=&#34;core-concepts-핵심-개념:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Core Concepts (핵심 개념)&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;machine-learning-기계-학습:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Machine Learning (기계 학습)&lt;/h3&gt;

&lt;p&gt;기계 학습을 통해 우리는 (1) 데이터를 획득하여, (2) 데이터를 통해 모델을 학습하고, (3) 학습된 모델을 이용하여, 새로운 데이터에 대해 예측한다. 모델을 &lt;a href=&#34;http://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training#training&#34;&gt;학습&lt;/a&gt;하는 과정은 새롭고, 익숙치 않은 자료를 하나씩 살펴보고 배움을 얻는 과정과도 같다. 각 단계마다, 모델은 예측을 하고, 얼마나 정확히 예측하였는지에 대해 피드백을 받는다. 이 피드백은 정답으로부터 얼만큼 차이가 나는지 등과 같은 방법을 통해 측정 가능한 오류(error)를 통해, 예측을 더 정확하게 하는데 사용된다.&lt;/p&gt;

&lt;p&gt;학습과정은 종종 파라미터 공간 (parameter space)에서 후진-전진이 반복는 게임이다: 만약 당신이 좋은 예측 결과를 얻기 위해 모델의 파라미터를 수정한다면, 이전에 제대로 예측했던 것도 수정 이후, 틀리게 예측될 수도 있다. 우수한 예측 성능을 가진 모델을 학습한다는 것은 많은 반복 작업이 필요할 것이다. 이런 반복적인 예측-수정의 과정은 예측 결과가 더 이상 발전이 없을 때 까지 반복한다.&lt;/p&gt;

&lt;h3 id=&#34;feature-engineering-특징-공학:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Feature Engineering (특징 공학)&lt;/h3&gt;

&lt;p&gt;특징 공학은 &lt;a href=&#34;https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-core-concepts/#machine-learning&#34;&gt;기계 학습&lt;/a&gt; 모델로 하여금 데이터들로 부터 클래스들을 쉽게 구분 할 수 있도록 도와주는 유용한 패턴을 추출 하는 것이다. 예를 들어,사진으로 부터 땅과 물고기를 구분하는 것을 초록색과 푸른색 픽셀의 수를 이용한다고 하자. 이런 특징은 기계 학습 모델에 도움이 된다. 좋은 분류를 위해 클래스의 갯수가 제한되어 있기때문에 좋은 분류가 가능하다.&lt;/p&gt;

&lt;p&gt;특징 공학은 대부분의 예측 분야에서 좋은 성능을 얻기위해 요구되는 가장 중요한 기술이다. 하지만, 다른 데이터 셋과 다른 종류의 데이터들은 각자 다른 특징 공학 기법이 필요하기에, 최고의 특징 공학 기술을 습득하고 마스터하기엔 어렵다. 특징 공학은 과학이라기 보다 예술의 경지에 가깝다. 특정 데이터 셋에서 추출된 특징은 종종 다른 데이터 셋에서는 적용되지 않는다. (위 예제에서 계속하여, 다음 사진이 오직 육지 동물만 포함하는 경우). 특징 공학이 어렵다는 점과 많은 노력이 요구되는 점이 &lt;strong&gt;자동으로 특징을 학습할 수 있는 알고리즘&lt;/strong&gt;을 찾게 되는 가장 큰 이유이다.&lt;/p&gt;

&lt;p&gt;물체 인식이나 음성 인식과 같은 분야의 영역이 특징 학습을 통해 자동화되고 있지만, 특징 공학은 &lt;a href=&#34;http://blog.kaggle.com/2014/08/01/learning-from-the-best/&#34;&gt;kaggle 기계 학습 대회의 어려운 여러 분야에서 가장 효율적인 방법&lt;/a&gt;으로  여전히 지속 될 것이다.&lt;/p&gt;

&lt;h3 id=&#34;feature-learning-특징-학습:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Feature Learning (특징 학습)&lt;/h3&gt;

&lt;p&gt;특징 학습 알고리즘은 서로 다른 범주(클래스)의 분류를 위한 가장 중요하며, 공통적으로 나타나는 패턴을 찾는다. 그리고 특징을 자동으로 추출하여 분류나 회귀 문제에 적용된다. 특징 학습은 특징 공학처럼 알고리즘을 통해 자동으로 수행되는 것 처럼 생각된다. 딥러닝에서는 convolutional layer가 예외적으로 이미지에서 좋은 특징을 찾는데 탁월한 능력을 보인다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2015/11/hierarchical_features.png&#34; alt=&#34;&#34; /&gt;
Figure 1: 딥 러닝 알고리즘으로 부터 학습된 계층형 특징들.&lt;/p&gt;

&lt;h4 id=&#34;deep-learning-딥-러닝:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Deep Learning (딥 러닝)&lt;/h4&gt;

&lt;h3 id=&#34;fundamental-concepts-기본-개념:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Fundamental Concepts (기본 개념)&lt;/h3&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;logistic-regression:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Logistic Regression ()&lt;/h4&gt;

&lt;h4 id=&#34;artificial-neural-network-인공-신경망:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Artificial Neural Network (인공 신경망)&lt;/h4&gt;

&lt;h4 id=&#34;unit:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Unit&lt;/h4&gt;

&lt;h4 id=&#34;artificial-neuron:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Artificial Neuron&lt;/h4&gt;

&lt;h4 id=&#34;activation-function:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Activation Function&lt;/h4&gt;

&lt;h4 id=&#34;layer:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Layer&lt;/h4&gt;

&lt;h3 id=&#34;convolutional-deep-learning:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Convolutional Deep Learning&lt;/h3&gt;

&lt;hr /&gt;

&lt;h4 id=&#34;convolution:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Convolution&lt;/h4&gt;

&lt;h4 id=&#34;pooling-subsampling:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Pooling / Subsampling&lt;/h4&gt;

&lt;h4 id=&#34;convolutional-neural-network-cnn:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Convolutional Neural Network (CNN)&lt;/h4&gt;

&lt;h4 id=&#34;inception:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Inception&lt;/h4&gt;

&lt;h3 id=&#34;conclusion-to-part-1:94c226eaa944803673f4f82b27d8c24f&#34;&gt;Conclusion to Part 1&lt;/h3&gt;

&lt;hr /&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>https://tgjeon.github.io/home/about/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://tgjeon.github.io/home/about/</guid>
      <description>

&lt;h2 id=&#34;biography:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;Biography&lt;/h2&gt;

&lt;p&gt;As research scientist, my focus has been on identifying technologies that further healthcare based on machine learning. I have spent the last 7 years for diagnosing cardiac diseases (Atrial Fibrillation, Myocardial Infaction, Ventricular Tachycardia, and Ventricular Fibrillation) and robust heartbeat detection from multimodal physiological signals (ECG, EEG, BP, EMG, and Respiration).
Also, I have designed and developed a prototype based on AM335x ARM Cortex-A8 and implemented device driver and numerical programming for real-time ECG processing and analysis.
My current research focuses on stream processing and analytics based on deep learning for the biometric data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Robust Detection of Heart Beats using Association Models from BP and EEG signals</title>
      <link>https://tgjeon.github.io/publication/robust_detection_of_heartbeats/</link>
      <pubDate>Fri, 15 Jan 2016 01:26:13 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/robust_detection_of_heartbeats/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Iterative Feature Selection Method for Shock Advice Algorithm with Artifact-Free ECG</title>
      <link>https://tgjeon.github.io/publication/iterative_feature_selection/</link>
      <pubDate>Wed, 16 Dec 2015 01:52:27 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/iterative_feature_selection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Implementation of portable device for real-time ECG signal analysis</title>
      <link>https://tgjeon.github.io/publication/implementation_of_portable_device/</link>
      <pubDate>Wed, 10 Dec 2014 01:43:07 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/implementation_of_portable_device/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Heart beat detection method with estimation of regular intervals between ECG and blood pressure</title>
      <link>https://tgjeon.github.io/publication/heartbeat_detection_method_with_estimation_of_regular_intervals/</link>
      <pubDate>Sun, 07 Sep 2014 09:15:02 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/heartbeat_detection_method_with_estimation_of_regular_intervals/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Heart Beat Detection Method using Estimated Regular Intervals from ECG and Blood Pressure</title>
      <link>https://tgjeon.github.io/publication/heartbeat_detection_method_with_estimated_regular_intervals_from_ECG_and_BP/</link>
      <pubDate>Sun, 22 Jun 2014 09:21:50 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/heartbeat_detection_method_with_estimated_regular_intervals_from_ECG_and_BP/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Heart Beat Detection Method using Heterogeneous Physiological Signal Analysis</title>
      <link>https://tgjeon.github.io/publication/heartbeat_detection_method_using_heterogeneous_physiological_signal_analysis/</link>
      <pubDate>Wed, 12 Mar 2014 09:24:54 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/heartbeat_detection_method_using_heterogeneous_physiological_signal_analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Developing Detection Algorithms of Heart Diseases and Portable ECG</title>
      <link>https://tgjeon.github.io/publication/developing_detection_algorithms_of_heart_diseases_and_portable_ECG/</link>
      <pubDate>Thu, 24 Jan 2013 09:31:37 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/developing_detection_algorithms_of_heart_diseases_and_portable_ECG/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Region based Scene Segmentation method for Topography Analysis</title>
      <link>https://tgjeon.github.io/publication/region_based_scene_segmentation_method_for_topography_analysis/</link>
      <pubDate>Thu, 22 Nov 2012 09:37:32 +0900</pubDate>
      
      <guid>https://tgjeon.github.io/publication/region_based_scene_segmentation_method_for_topography_analysis/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>